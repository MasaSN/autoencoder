{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets , transforms\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Convolution3D, MaxPooling3D, UpSampling3D\n",
    "from pyexpat import model\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch.optim\n",
    "import torch.optim.lr_scheduler as ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(items):\n\u001b[0;32m      8\u001b[0m     item_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, items[i])\n\u001b[1;32m----> 9\u001b[0m     item_data \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     item_array \u001b[38;5;241m=\u001b[39m item_data\u001b[38;5;241m.\u001b[39mget_fdata()\n\u001b[0;32m     11\u001b[0m     item_array \u001b[38;5;241m=\u001b[39m item_array[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m128\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\loadsave.py:108\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m sniff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_klass \u001b[38;5;129;01min\u001b[39;00m all_image_classes:\n\u001b[1;32m--> 108\u001b[0m     is_valid, sniff \u001b[38;5;241m=\u001b[39m \u001b[43mimage_klass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_maybe_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msniff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n\u001b[0;32m    110\u001b[0m         img \u001b[38;5;241m=\u001b[39m image_klass\u001b[38;5;241m.\u001b[39mfrom_filename(filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\filebasedimages.py:476\u001b[0m, in \u001b[0;36mFileBasedImage.path_maybe_image\u001b[1;34m(klass, filename, sniff, sniff_max)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sniff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sniff[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m klass\u001b[38;5;241m.\u001b[39m_meta_sniff_len:\n\u001b[0;32m    475\u001b[0m     sniff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m sniff \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sniff_meta_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta_sniff_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msniff_max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msniff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sniff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sniff[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m<\u001b[39m klass\u001b[38;5;241m.\u001b[39m_meta_sniff_len:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, sniff\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\filebasedimages.py:421\u001b[0m, in \u001b[0;36mFileBasedImage._sniff_meta_for\u001b[1;34m(klass, filename, sniff_nbytes, sniff)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# Attempt to sniff from metadata location\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 421\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImageOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    422\u001b[0m         binaryblock \u001b[38;5;241m=\u001b[39m fobj\u001b[38;5;241m.\u001b[39mread(sniff_nbytes)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m COMPRESSION_ERRORS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mEOFError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\openers.py:182\u001b[0m, in \u001b[0;36mOpener.__init__\u001b[1;34m(self, fileish, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Clear keep_open hint if it is not relevant for the file type\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeep_open\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfobj \u001b[38;5;241m=\u001b[39m \u001b[43mopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m fileish\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mme_opened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\openers.py:91\u001b[0m, in \u001b[0;36m_gzip_open\u001b[1;34m(filename, mode, compresslevel, mtime, keep_open)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gzip_open\u001b[39m(\n\u001b[0;32m     83\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     84\u001b[0m     mode: Mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m     keep_open: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m gzip\u001b[38;5;241m.\u001b[39mGzipFile:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m HAVE_INDEXED_GZIP \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m         gzip_file \u001b[38;5;241m=\u001b[39m \u001b[43mDeterministicGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# use indexed_gzip if possible for faster read access.  If keep_open ==\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# True, we tell IndexedGzipFile to keep the file handle open. Otherwise\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# the IndexedGzipFile will close/open the file on each read.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m         gzip_file \u001b[38;5;241m=\u001b[39m IndexedGzipFile(filename, drop_handles\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m keep_open)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nibabel\\openers.py:72\u001b[0m, in \u001b[0;36mDeterministicGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMust define either fileobj or filename\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Cast because GzipFile.myfileobj has type io.FileIO while open returns ty.IO\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m ty\u001b[38;5;241m.\u001b[39mcast(io\u001b[38;5;241m.\u001b[39mFileIO, \u001b[38;5;28mopen\u001b[39m(filename, modestr))\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     74\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     75\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmodestr,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     mtime\u001b[38;5;241m=\u001b[39mmtime,\n\u001b[0;32m     79\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "# DONT RUN THIS CELL\n",
    "path = r\"D:\\DELL\\Documents\\final_output\"\n",
    "output_dir = r'D:\\DELL\\Documents\\filtered_data'\n",
    "items = os.listdir(path)\n",
    "clean_list =[]\n",
    "for i,item in enumerate(items):\n",
    "    item_name = os.path.join(path, items[i])\n",
    "    item_data = nib.load(item_name)\n",
    "    item_array = item_data.get_fdata()\n",
    "    item_array = item_array[0:128]\n",
    "    item_array = torch.tensor(item_array)\n",
    "    if(item_array.shape == torch.Size([128, 240, 256])):\n",
    "        #clean_list.append(item_array)\n",
    "        shutil.move(item_name, os.path.join(output_dir, item))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n",
      "torch.Size([128, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "for item in clean_list:\n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'D:\\DELL\\Documents\\filtered_data'\n",
    "\n",
    "class Patient_Dataset(Dataset):\n",
    "  def __init__(self, transform = None):\n",
    "      self.root_dir = output_dir\n",
    "      self.transform = transform\n",
    "      self.nifti_files = os.listdir(output_dir)\n",
    "      self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.nifti_files)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "     nifti_name = os.path.join(self.root_dir,self.nifti_files[idx])\n",
    "     nifti_data = nib.load(nifti_name)\n",
    "     nifti_array = nifti_data.get_fdata()\n",
    "     nifti_array = nifti_array[0:128]\n",
    "     \n",
    "    # nifti_array = clean_list[idx]\n",
    "\n",
    "        # Resize the NIfTI data\n",
    "\n",
    "\n",
    "        # Stack the resized slices\n",
    "      #resized_data = np.stack(resized_data)\n",
    "\n",
    "     if self.transform:\n",
    "          nifti_array = self.transform(nifti_array)\n",
    "\n",
    "     else:\n",
    "          nifti_array = torch.tensor(nifti_array)\n",
    "          nifti_array = nifti_array.unsqueeze(0)\n",
    "     return nifti_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for images in data_loader:\\n\\n    # Print the minimum and maximum values of the images in the batch\\n    print(torch.min(images), torch.max(images))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor()   # there might be something with this function that switchs the slice index with the hieght index\n",
    "])\n",
    "dataset = Patient_Dataset( )#,transform=data_transform)\n",
    "data_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "print(len(dataset))\n",
    "\n",
    "'''for images in data_loader:\n",
    "\n",
    "    # Print the minimum and maximum values of the images in the batch\n",
    "    print(torch.min(images), torch.max(images))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1412\\3658009883.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nifti_array = torch.tensor(nifti_array)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "106\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "j=0\n",
    "t =0\n",
    "for data in dataset:\n",
    "    if data.shape == torch.Size([128, 256, 256]):\n",
    "        i+=1\n",
    "    elif data.shape == torch.Size([128, 240, 256]):\n",
    "        j+=1\n",
    "    elif data.shape == torch.Size([128, 192, 192]):\n",
    "        dataset.__delattr__\n",
    "        t+=1\n",
    "    else:\n",
    "        print(\"a weird data shape\")  \n",
    "        print(data.shape)\n",
    "\n",
    "print(i)\n",
    "print(j)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized data shape: torch.Size([1, 128, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[3]  # Get the first sample from the dataset\n",
    "print(\"Resized data shape:\", sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 240, 256, 128])\n",
      "torch.Size([1, 16, 30, 32, 16])\n",
      "torch.Size([1, 1, 240, 256, 128])\n"
     ]
    }
   ],
   "source": [
    "class Encoder3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder3D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1),  # Output: 8x28x28x28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),  # Output: 8x14x14x14\n",
    "            nn.Conv3d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1),  # Output: 16x14x14x14  avoid maxpooling\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2),  # Output: 16x7x7x7\n",
    "            nn.Conv3d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=2),  # Output: 16x7x7x7    research leaky relu gelu\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=1),  # Output: 16x6x6x6     need normalization maybe random dropout or batch normalization and\n",
    "            # note : we also changed the stride to so the output is an integer\n",
    "            #nn.Conv3d(in_channels=16, out_channels=3, kernel_size=3, stride=1, padding=1)  # Output: 3x4x4x4\n",
    "\n",
    "            # for calculating the output of a MaxPool layer we use (n - w)/s +1\n",
    "            # n = input size\n",
    "            # w = kernel size\n",
    "            # s = stride size\n",
    "\n",
    "            # for calculating the output size of a convLayer we use (((W - K + 2P) / S) + 1)\n",
    "            # n = input size\n",
    "            # w = kernel size\n",
    "            # s = stride size\n",
    "            # p = padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance\n",
    "encoder_3d = Encoder3D()\n",
    "\n",
    "x_3d = torch.randn(1,1,240, 256, 128)\n",
    "#x_3d = torch.randn(20, 1, 28, 28, 28)\n",
    "y_3d = encoder_3d(x_3d)\n",
    "print(x_3d.shape)\n",
    "print(y_3d.shape)  # torch.Size([20, 3, 4, 4, 4]) the same as the last conv layer\n",
    "\n",
    "class Decoder3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder3D, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1,output_padding=1,dilation=1),\n",
    "            #nn.ConvTranspose3d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=0),  # Output: 16x9x9x9\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose3d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1, output_padding=1,dilation=1),  # Output: 8x18x18x18\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose3d(in_channels=8, out_channels=1, kernel_size=3, stride=2, padding=2, output_padding= 1, dilation=2),  # Output: 4x36x36x36\n",
    "            nn.ReLU()\n",
    "            #nn.ConvTranspose3d(in_channels=4, out_channels=1, kernel_size=3, stride=1, padding=5)  # Output: 1x28x28x28\n",
    "\n",
    "            # the equation for the output of transpose convLayers is (W−1)*S − 2×P + dilation ×(K−1) + output_padding +1\n",
    "            # W = Input size\n",
    "            # K = Size of kernels (Filter size)\n",
    "            # S = Stride\n",
    "            # P = Padding\n",
    "            # By\n",
    "            # default, stride = 1, padding = 0, dilation = 1, and output_padding = 0\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance\n",
    "decoder_3d = Decoder3D()\n",
    "\n",
    "x_decoded_3d = decoder_3d(y_3d) # y_3d is the encoded output from the encoder\n",
    "print(x_decoded_3d.shape)  # torch.Size([20, 1, 28, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummaryNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 30, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, padding=1),\n",
    "            nn.Conv3d(30, 60, kernel_size=5, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool3d(kernel_size=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ConvTranspose3d(60, 30, kernel_size=5, padding=3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ConvTranspose3d(30,  1, kernel_size=5, padding=2),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "sample_tensor = torch.randn( 1,128, 240, 256)\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "#reconstructed_tensor = autoencoder(sample_tensor)\n",
    "\n",
    "#print(\"Shape of reconstructed tensor:\", reconstructed_tensor.shape)\n",
    "#print(autoencoder)\n",
    "#summary(autoencoder, ( 1,128, 240, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder3D, self).__init__()\n",
    "        self.encoder = Encoder3D()\n",
    "        self.decoder = Decoder3D()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss maybe mae will work better\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "import torch.optim.lr_scheduler as ll\n",
    "\n",
    "scheduler = ll.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 106, Loss: 167.0439\n",
      "Epoch: 2, Iteration: 106, Loss: 422.9420\n",
      "Epoch: 3, Iteration: 106, Loss: 250.7212\n",
      "Epoch: 4, Iteration: 106, Loss: 378.6319\n",
      "Epoch: 5, Iteration: 106, Loss: 213.4715\n",
      "Epoch: 6, Iteration: 106, Loss: 72.0956\n",
      "Epoch: 7, Iteration: 106, Loss: 89.6297\n",
      "Epoch: 8, Iteration: 106, Loss: 502.9212\n",
      "Epoch: 9, Iteration: 106, Loss: 188.3651\n",
      "Epoch: 10, Iteration: 106, Loss: 238.4381\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "outputs =[]\n",
    "for epoch in range(num_epochs):\n",
    "    for idx, img in enumerate(data_loader):\n",
    "        img = img.float()  # Convert input tensor to float\n",
    "\n",
    "        recon = autoencoder(img)\n",
    "        loss = criterion(recon, img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    print(f'Epoch: {epoch+1}, Iteration: {idx+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    outputs.append((epoch, img, recon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 172\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_loader\u001b[49m:\n\u001b[0;32m      5\u001b[0m         img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Convert input tensor to float\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         recon \u001b[38;5;241m=\u001b[39m autoencoder(img)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "outputs =[]\n",
    "for epoch in range(num_epochs):\n",
    "    for img in data_loader:\n",
    "        img = img.float()  # Convert input tensor to float\n",
    "\n",
    "        recon = autoencoder(img)\n",
    "        loss = criterion(recon,img)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1} , loss {loss.item():.4f}')\n",
    "    outputs.append((epoch,img,recon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
